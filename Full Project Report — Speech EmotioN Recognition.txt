Full Project Report — Speech Emotion Recognition (Mixed English/Hindi)

Note: नीचे मैं notebook के सामान्य और सबसे ज़्यादा use होने वाले code blocks और उनसे जुड़े line-by-line explanations दे रहा हूँ — यह आपकी uploaded notebook के 32 cells के flow के अनुरूप है (mount → imports → data load → feature extraction → preprocessing → train/test → modeling → evaluation → save). अगर किसी specific cell में कुछ अलग code है तो बताओ और मैं उसे detail में explain कर दूँगा।

1) Project Overview — क्या है, कैसे काम करता है (Concept)

English: Speech Emotion Recognition (SER) is the task of automatically detecting the emotional state of a speaker from audio (speech) signals.
Hindi: यह system audio files को लेकर उनमे छुपी भावनाएँ (like happy, sad, angry, neutral, fearful, etc.) पहचानता है।

Core idea:

Convert raw audio → extract informative features (e.g., MFCCs, chroma, spectral features) → feed to ML/DL model → predict emotion label.

Datasets often used: RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song), CREMA-D, SAVEE, Emo-DB — each structured as audio files in folders or actor-wise.

2) Notebook high-level flow (cell-by-cell summary)

I'll describe the typical cell types and explain lines inside each. Use this as a line-to-line guide.

Cell 1 — Drive mount (Colab)
from google.colab import drive
drive.mount('/content/drive')


from google.colab import drive — Colab-specific import.

drive.mount('/content/drive') — mounts your Google Drive so you can read dataset stored there.

Hindi: अगर dataset local है तो mount जरूरी है; local Jupyter में यह line नहीं चाहिए।

Cell 2 — Imports (libraries)

Typical lines:

import os
import numpy as np
import pandas as pd
import librosa
import soundfile
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import pickle
import matplotlib.pyplot as plt
import seaborn as sns


Explain line-by-line (short):

os — file/folder operations (listdir, join).

numpy as np — numerical arrays.

pandas as pd — dataframes for features/labels.

librosa — dominant audio library: load audio, compute MFCC, chroma, mel-spectrogram etc.

soundfile — alternate audio I/O.

sklearn.model_selection.train_test_split — split dataset.

StandardScaler — normalize features.

LabelEncoder — convert emotion strings to integers.

SVC — SVM classifier (example).

metrics — evaluate model.

pickle — save trained model.

matplotlib/seaborn — plots.

Hindi tip: librosa.load(file, sr=None) loads audio; sr=None keeps original sample rate.

Cell 3 — Dataset path & listing files

Typical lines:

data_path = '/content/drive/MyDrive/RAVDESS/'
files = []
for root, dirs, filenames in os.walk(data_path):
    for file in filenames:
        if file.endswith('.wav'):
            files.append(os.path.join(root, file))
len(files)


os.walk(data_path) recursively lists files.

Filter .wav files and collect full paths.

Hindi: RAVDESS में audio filenames अक्सर actor_emotion_sentence.ext होते हैं — filenames से emotion extract कर सकते हैं (parsing).

Cell 4 — Parsing labels from filenames (RAVDESS style)

Example:

# Example RAVDESS filename: 'Actor_01/03-01-05-02-02-02-01.wav'
# Here emotion code '05' corresponds to 'happy' (depends on dataset docs)
def parse_emotion_from_filename(file_path):
    fname = os.path.basename(file_path)
    parts = fname.split('-')  # depends on naming
    emotion_code = int(parts[2])  # example index
    return emotion_map[emotion_code]


Many datasets encode emotion in filename; create emotion_map dict mapping code → label.

Hindi: अगर filenames clear नहीं हैं, कुछ datasets have CSV metadata which should be used.

Cell 5 — Feature extraction function (core)

Typical pattern:

def extract_features(file_name):
    X, sample_rate = librosa.load(file_name, sr=22050)  # load audio
    # 1. MFCC
    mfccs = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40)
    mfccs_mean = np.mean(mfccs.T, axis=0)
    # 2. Chroma
    stft = np.abs(librosa.stft(X))
    chroma = librosa.feature.chroma_stft(S=stft, sr=sample_rate)
    chroma_mean = np.mean(chroma.T, axis=0)
    # 3. Mel
    mel = librosa.feature.melspectrogram(X, sr=sample_rate)
    mel_mean = np.mean(mel.T, axis=0)
    # Combine
    return np.hstack([mfccs_mean, chroma_mean, mel_mean])


Line-by-line explanation:

librosa.load(..., sr=22050) — loads and resamples audio (22050 Hz commonly used).

librosa.feature.mfcc(...) — calculates MFCC matrix: shape (n_mfcc, frames); we take mean across time np.mean(..., axis=0) to get fixed-length vector.

librosa.stft -> chroma_stft — gets chroma features (pitch class profile).

melspectrogram — energy in mel bands.

np.hstack([...]) — join all features to get final feature vector for that file.

Hindi: feature extraction ही सबसे important step है — better features → better model. MFCCs speech ke timbral properties capture karte hain; chroma useful for pitch; mel-spectrogram more general.

Cell 6 — Build feature matrix + labels

Typical code:

features = []
labels = []
for f in files:
    emotion = parse_emotion_from_filename(f)
    feat = extract_features(f)
    features.append(feat)
    labels.append(emotion)
X = np.array(features)
y = np.array(labels)


X will be shape (num_samples, feature_dim); y is (num_samples,).

Pitfall: some files may be corrupted or too short — add try/except to skip and log them.

Cell 7 — Encoding labels + scaling
le = LabelEncoder()
y_encoded = le.fit_transform(y)  # string -> integers
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


LabelEncoder converts emotion names to ints (e.g., angry→0, happy→1).

StandardScaler mean=0, variance=1 scaling (improves many models).

Hindi: SVM/NNs sensitive to feature scaling, so always scale.

Cell 8 — Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)


stratify=y_encoded keeps class distribution same in train/test.

Tip: for speaker-independent evaluation, split by actors (RAVDESS actors) not random split — otherwise model might overfit voice of same actor.

Cell 9 — Model training (example SVM)
clf = SVC(kernel='rbf', probability=True)
clf.fit(X_train, y_train)


SVC with RBF kernel — works well for moderate feature dims.

Alternative models: RandomForestClassifier, MLPClassifier, KNeighborsClassifier, or deep models (CNNs, RNN/LSTM).

Hindi: small dataset → classical ML often enough; large → deep learning.

Cell 10 — Evaluate model
y_pred = clf.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred, target_names=le.classes_))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', xticklabels=le.classes_, yticklabels=le.classes_)
plt.show()


classification_report shows precision/recall/F1 per class.

Confusion matrix shows which emotions confused.

Important note: For imbalanced data, accuracy is misleading; prefer F1-score (macro or weighted).

Cell 11 — Save model + scaler + label encoder
with open('model.pkl', 'wb') as f:
    pickle.dump(clf, f)
with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)
with open('le.pkl', 'wb') as f:
    pickle.dump(le, f)


Save objects to disk for later inference.

Hindi: use joblib or pickle; for large models prefer joblib.dump.

Cell 12 — Demo: predict on new audio
def predict_emotion(file_path):
    feat = extract_features(file_path)
    feat_scaled = scaler.transform([feat])
    pred = clf.predict(feat_scaled)
    label = le.inverse_transform(pred)
    return label[0]


scaler.transform([feat]) — remember to use same scaler used when training.

Pitfall: If feature dimensions mismatch (e.g., extract_features gives different length), prediction will fail.

Additional cells: plots, EDA, duration distribution

Plot distribution of emotions, audio durations, sample waveforms, spectrograms.

Visual checks help spot dataset imbalance.

3) Line-by-line: common gotchas and what each important function does

librosa.load(path, sr=22050) — loads waveform (numpy array y) and sampling rate sr. If sr=None, keeps original sampling rate.

librosa.feature.mfcc(y, sr, n_mfcc=40) — returns (n_mfcc x frames) matrix. Usually we take mean/std/deltas to reduce time dimension.

librosa.feature.delta(mfccs) — compute delta (derivative) features to capture dynamics.

np.mean(matrix.T, axis=0) — mean across time frames → 1D vector.

np.hstack([a, b, c]) — concatenate 1D vectors into final feature vector.

LabelEncoder().fit_transform(labels) — string→ int mapping.

train_test_split(..., stratify=y) — preserves class distribution.

scaler.fit_transform(X) vs scaler.transform(X_new) — always fit on training set only.

clf.fit(X_train, y_train) — trains the model.

clf.score vs accuracy_score — both measure accuracy but follow same concept.

4) Features used (what they capture) — English + Hindi

MFCC (Mel-Frequency Cepstral Coefficients)

English: captures timbral, short-term spectral envelope of speech.

Hindi: आवाज़ का “tone/timbre” और formant information capture karta hai.

Chroma

English: pitch-class (useful for music/pitch).

Hindi: speech में pitch variations ka kuch info deta hai (helpful in prosody).

Mel-spectrogram / Spectral features

English: energy distribution in mel-bands.

Hindi: loudness/energy patterns, helpful for excited vs calm.

Zero Crossing Rate, Spectral Centroid, Roll-off (if used)

capture noisiness, brightness, etc.

Deltas (first and second derivative)

capture temporal dynamics.

Recommendation: Combine MFCC + delta + chroma + mel → 100–200 dimensional vector typically.

5) Advantages of this approach

Uses well-understood audio features (MFCCs) — robust for speech tasks.

Classical ML models (SVM/RF) are fast to train and work well on small datasets.

Easy to interpret (confusion matrices show per-class errors).

Can be deployed offline (no big GPUs required).

Hindi: small dataset par bhi achha performance milta hai agar features सही हों।

6) Difficulties & pitfalls you may face (and fixes)

Dataset size & imbalance

Problem: Emotions often imbalanced → biased model.

Fix: data augmentation (time-stretch, pitch-shift, add noise), class weighting, oversampling (SMOTE) carefully, or use balanced sampling.

Overfitting (speaker-dependent)

Problem: Random split may allow same actor in train & test → over-optimistic accuracy.

Fix: speaker-independent split (split by actor ID), cross-validation by actor.

Noisy/short audio files

Problem: very short clips produce poor features.

Fix: skip extremely short files, pad/truncate to fixed length.

Feature mismatch / shapes

Problem: variable-length features if you don't aggregate (mean).

Fix: always derive fixed-length vector per audio (mean+std or fixed frames).

Different sample rates

Problem: mixing sr → inconsistent features.

Fix: choose a common sr (e.g., 22050) and resample all.

Emotion ambiguity & subjectivity

Human-labeled emotions can be noisy — label noise hurts supervised learning.

Choosing right model

Nonlinear kernels or deep models? For small datasets SVM/RandomForest/MLP are safer. For bigger datasets, CNN on spectrograms or LSTM on sequences performs better.

Real-time latency

If you need real-time prediction, feature extraction and model must be optimized (use lower n_mfcc, use faster models, multithreading).

Language and accent variability

Models trained on English actors (RAVDESS) may not generalize to other languages or accents.

7) Which code does what — quick mapping (cheat-sheet)

librosa.load → audio reading

librosa.feature.mfcc → MFCC extraction

np.mean(..., axis=0) → time aggregation → fixed vector

np.hstack([...]) → combine features

LabelEncoder → labels → ints

StandardScaler → normalize features

train_test_split → split data

SVC.fit / RandomForest.fit / MLP.fit → train model

predict → run inference

confusion_matrix / classification_report → evaluate

pickle.dump → persist model/scaler/encoder

8) Example: Common errors and exactly how to debug (practical)

Error: ValueError: setting an array element with a sequence.

Cause: feature vectors have inconsistent lengths.

Fix: ensure extract_features returns fixed-size vector (use mean/std or pad/truncate).

Error: FileNotFoundError when listing dataset.

Fix: check data_path string, confirm mounted drive and correct folder name.

Error: shapes (1, 40) and (50,) not aligned during scaling or model input.

Fix: scaler.transform([feat]) not scaler.transform(feat); ensure 2D input array.

Low accuracy / Naive model:

Try cross-validation, inspect confusion matrix for specific classes, augment data, tune hyperparams.

librosa errors reading file:

Some files may be corrupt; wrap librosa.load in try/except and log fails.

9) How to improve model (advanced / next steps)

Use spectrogram images + CNN

Convert audio to mel-spectrogram images and train a CNN (transfer learning with ResNet, EfficientNet on spectrograms).

Use sequential models (RNN/LSTM/GRU)

Keep time dimension (sequence of frames) and use LSTMs/transformers to model temporal dependencies.

Data augmentation

Pitch shift, time stretch, additive noise, room reverberation.

Multi-modal (audio + video)

If dataset has video (RAVDESS contains video), fuse facial expression + audio for better predictions.

Speaker normalization

Remove speaker-specific traits (vocal tract length normalization) to make emotion-general features.

Use pretrained audio embeddings

Use models like YAMNet, OpenL3, Wav2Vec2 for powerful embeddings and then a classifier on top.

Hyperparameter tuning

GridSearchCV or Bayesian optimization for SVM C/gamma, RF number of trees, NN architectures.

10) Example final report structure (you can copy-paste into a file)

(Provide this as text you can paste into your project report.)

Title: Speech Emotion Recognition using Audio Features (MFCC, Chroma, Mel)
Author: Roman (or your name)
Date: [add date]

Abstract:
Short summary: what, dataset, approach, summary of results (accuracy, per-class F1).

1. Introduction — background on emotion recognition, applications (call center sentiment monitoring, mental health detection, human-robot interaction).

2. Dataset — describe RAVDESS: number of actors, emotions, sample rate, folder structure. (List exact path used.)

3. Methodology

Data pre-processing: resampling, trimming/padding.

Features: MFCC (n=40), Chroma (12), Mel (128) — describe each mathematically (brief).

Model: SVM (RBF), hyperparameters.

Training: train/test split ratio, stratify, cross-validation.

4. Implementation (Line-by-line)

Include the main functions and explanations (I already gave above; paste here).

5. Results

Accuracy, confusion matrix, classification report.

Show plots (emotion distribution, sample spectrogram).

6. Discussion

Which classes confused, why; speaker dependency; limitations.

7. Future Work

Spectrogram CNNs, multi-modal, augmentation, larger datasets.

8. Conclusion

Appendix A: Code snippets

Provide extract_features, predict_emotion, training script, saving/loading model.

11) Suggested improvements to your notebook (practical edits)

Add try/except around audio load to skip bad files and log them.

Save features+labels to a CSV (pd.DataFrame) so you avoid recomputing features every run.

Do speaker-independent split for more realistic evaluation: extract actor id from filename and split actors between train/test.

Use class_weight='balanced' (for scikit-learn classifiers) if imbalanced.

Add cross-validation and report mean+std accuracy.

Visualize misclassified examples: print audio filename + true vs predicted for top confusions.

12) Short checklist before running the notebook (practical)

Mount drive / set data_path correctly.

Ensure librosa, soundfile, scikit-learn, pandas, numpy installed.

Confirm dataset has .wav files and their naming convention.

Run feature extraction on a subset first to validate shapes.

Save feature matrix to disk once computed.

13) Quick code debugging snippets you can drop into notebook

Safe load audio:

def safe_extract(file_path):
    try:
        feat = extract_features(file_path)
        return feat
    except Exception as e:
        print("Failed:", file_path, "err:", e)
        return None


Ensure fixed-length feature:

feat = extract_features(file)
if feat is None or len(feat) != expected_dim:
    print("Bad feature for", file)


Save features to CSV:

df = pd.DataFrame(X)
df['label'] = y
df.to_csv('features.csv', index=False)